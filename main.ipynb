{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea3f071-036e-472b-a6dc-67900e305961",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/rsna-breast-cancer-detection/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/rsna-breast-cancer-detection/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/rsna-breast-cancer-detection/train.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3bc69-fee4-425c-b03c-e83fcef05a63",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db58f8-f6f4-46a7-968c-ff2fb1962550",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"age\"] = train_df[\"age\"].fillna(train_df[\"age\"].median())\n",
    "#train_df[\"BIRADS\"] = train_df[\"BIRADS\"].fillna(train_df[\"BIRADS\"].median())\n",
    "#train_df = train_df.drop(columns=[\"BIRADS\"])\n",
    "#train_df = train_df.drop(columns=[\"density\"])\n",
    "train_df = train_df.drop(columns=[\"machine_id\"])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b64fbe-00db-40dc-b6d9-386b697895b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the base directory where images are stored\n",
    "BASE_DIR = \"/kaggle/input/rsna-breast-cancer-detection/train_images\"\n",
    "\n",
    "# Function to construct full image path\n",
    "def get_image_path(row):\n",
    "    return os.path.join(BASE_DIR, str(row[\"patient_id\"]), f\"{row['image_id']}.dcm\")\n",
    "\n",
    "# Apply the function to each row\n",
    "train_df[\"image_path\"] = train_df.apply(get_image_path, axis=1)\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e800ad-df9d-44b1-9dc2-270bcfcd1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base_path = \"/kaggle/input/rsna-breast-cancer-detection/train_images\"\n",
    "\n",
    "train_df['image_path'] = train_df.apply(\n",
    "    lambda row: f\"{image_base_path}/{row['patient_id']}/{row['image_id']}.dcm\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de3493-e7fa-495f-88c6-da1dcbc9c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.get_dummies(train_df, columns=['laterality', 'view'], drop_first=False)\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_cols = ['age']  # add more if needed\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "tabular_data = df.drop(columns=['site_id','invasive', 'biopsy', 'image_id','difficult_negative_case', 'patient_id', 'image_path', 'cancer', 'BIRADS', 'density'])\n",
    "tabular_data = tabular_data.astype(np.float32).values  # 👈 this is key!\n",
    "labels = df['cancer'].values\n",
    "labels = labels.astype(np.float32)  # Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56631e-eb99-4000-ad42-a920b85b3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.get_dummies(train_df, columns=['laterality', 'view'], drop_first=False)\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_cols = ['age']  # add more if needed\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Include all positive samples\n",
    "positive_df = df[df['cancer'] == 1]\n",
    "\n",
    "# Sample equal or smaller number of negative examples (or a 1:3 ratio)\n",
    "negative_df = df[df['cancer'] == 0].sample(n=len(positive_df) * 1, random_state=42)\n",
    "\n",
    "# Combine and shuffle\n",
    "df_balanced = pd.concat([positive_df, negative_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "tabular_data = df_balanced.drop(columns=['site_id','invasive', 'biopsy', 'image_id','difficult_negative_case', 'patient_id', 'image_path', 'cancer', 'BIRADS', 'density'])\n",
    "tabular_data = tabular_data.astype(np.float32).values  # 👈 this is key!\n",
    "labels = df_balanced['cancer'].astype(np.float32).values\n",
    "image_paths = df_balanced['image_path'].values.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a43a2a-45e6-4a09-8f40-5bf01564539f",
   "metadata": {},
   "source": [
    "# Model with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46342233-0d64-47e0-9dd7-cf1cb7b40749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications, Input\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Function to build the multimodal model\n",
    "def create_multimodal_model(image_shape=(224, 224, 1), tabular_input_dim=4):\n",
    "    # IMAGE INPUT BRANCH\n",
    "    image_input = layers.Input(shape=image_shape, name=\"image_input\")\n",
    "\n",
    "    # Convert grayscale image to RGB by using a Conv2D layer with 3 filters\n",
    "    x = layers.Conv2D(3, (1, 1), padding=\"same\")(image_input)  # Convert grayscale (1 channel) to 3 channels\n",
    "\n",
    "    # Use EfficientNetB0 without pre-trained weights\n",
    "    base_model = applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights=None,  # No pre-trained weights\n",
    "        input_tensor=x,  # Use the modified input tensor\n",
    "        pooling=\"avg\"  # Global average pooling\n",
    "    )\n",
    "\n",
    "    # Freeze the pre-trained layers (although there are none since we're training from scratch)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom layers on top of EfficientNetB0\n",
    "    x = base_model.output\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # TABULAR INPUT BRANCH\n",
    "    tabular_input = layers.Input(shape=(tabular_input_dim,), name=\"tabular_input\")\n",
    "    tabular_x = layers.Dense(64, activation=\"relu\")(tabular_input)\n",
    "    tabular_x = layers.Dropout(0.5)(tabular_x)\n",
    "\n",
    "    # Concatenate image and tabular branches\n",
    "    combined = layers.concatenate([x, tabular_x])\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(combined)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs=[image_input, tabular_input], outputs=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c3ac1-c3f2-4af9-aba0-2292c7cfa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(path):\n",
    "    # Decode the tensor path into a string\n",
    "    if isinstance(path, tf.Tensor):\n",
    "        path = path.numpy().decode(\"utf-8\")  # Convert from Tensor to string\n",
    "    \n",
    "    dicom_data = pydicom.dcmread(path)\n",
    "    image_array = dicom_data.pixel_array.astype(np.float32)\n",
    "    image_array = resize(image_array, (224, 224), mode='constant', preserve_range=True)\n",
    "    image_array = np.expand_dims(image_array, axis=-1)  # Add channel dim\n",
    "    image_array /= 255.0  # normalize\n",
    "    return image_array\n",
    "\n",
    "def preprocess(image_path, tabular_row, label):\n",
    "    image = tf.py_function(func=load_dicom_image, inp=[image_path], Tout=tf.float32)\n",
    "    image.set_shape([224, 224, 1])  # Set static shape\n",
    "    return (image, tabular_row), label\n",
    "\n",
    "\n",
    "def create_dataset(image_paths, tabular_data, labels, batch_size=32, shuffle=True):\n",
    "    image_paths = tf.constant(image_paths)\n",
    "    tabular_data = tf.convert_to_tensor(tabular_data, dtype=tf.float32)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, tabular_data, labels))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_img_train, X_img_val, X_tab_train, X_tab_val, y_train, y_val = train_test_split(\n",
    "    image_paths, tabular_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = create_dataset(X_img_train, X_tab_train, y_train, batch_size=32)\n",
    "val_ds = create_dataset(X_img_val, X_tab_val, y_val, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Build the model (make sure you define create_multimodal_model somewhere)\n",
    "model = create_multimodal_model(\n",
    "    image_shape=(224, 224, 1),  # CHANGED from 3 to 1 (grayscale)\n",
    "    tabular_input_dim=X_tab_train.shape[1]\n",
    ")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Train\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=6,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022926d-4b41-43a8-9b53-70ee43fe0f81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7666afd4-3865-4703-ae1c-dab7e4c6bcbf",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce27372-fc5f-4764-92c5-3e4d28a5a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ---------- 1. LOAD TEST METADATA ----------\n",
    "test_df = pd.read_csv(\"/kaggle/input/rsna-breast-cancer-detection/test.csv\")\n",
    "\n",
    "# ---------- 2. CREATE IMAGE PATHS ----------\n",
    "test_df[\"image_path\"] = test_df.apply(\n",
    "    lambda row: f\"/kaggle/input/rsna-breast-cancer-detection/test_images/{row['patient_id']}/{row['image_id']}.dcm\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------- 3. ONE-HOT ENCODE CATEGORICALS ----------\n",
    "test_df = pd.get_dummies(test_df, columns=[\"view\", \"laterality\"], drop_first=False)\n",
    "\n",
    "# ---------- 4. PREPARE FEATURES ----------\n",
    "# Modify this part to ensure 13 features are included\n",
    "tabular_features = [\"age\", \"implant\"] + [col for col in test_df.columns if col.startswith(\"view_\") or col.startswith(\"laterality_\")]\n",
    "X_tab_test = test_df[tabular_features].astype(np.float32).values\n",
    "\n",
    "# Check the shape of X_tab_test\n",
    "print(test_df)  # Should output (N, 13), where N is the number of samples.\n",
    "\n",
    "X_img_test_paths = test_df[\"image_path\"].values\n",
    "prediction_ids = test_df[\"prediction_id\"].values\n",
    "\n",
    "NUM_TABULAR_FEATURES = X_tab_test.shape[1]\n",
    "\n",
    "# ---------- 5. DICOM LOADING ----------\n",
    "def load_dicom_image(path):\n",
    "    if isinstance(path, tf.Tensor):\n",
    "        path = path.numpy().decode(\"utf-8\")\n",
    "\n",
    "    dicom = pydicom.dcmread(path)\n",
    "    img = dicom.pixel_array.astype(np.float32)\n",
    "    img = resize(img, (224, 224), mode='constant', preserve_range=True)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    img /= 255.0\n",
    "    return img\n",
    "\n",
    "# ---------- 6. TF.DATA PIPELINE ----------\n",
    "def preprocess_test(image_path, tabular_row):\n",
    "    image = tf.py_function(load_dicom_image, [image_path], tf.float32)\n",
    "    image.set_shape([224, 224, 1])\n",
    "    tabular_row.set_shape([NUM_TABULAR_FEATURES])\n",
    "    return (image, tabular_row)\n",
    "\n",
    "def create_test_dataset(image_paths, tabular_data, batch_size=32):\n",
    "    image_paths = tf.constant(image_paths)\n",
    "    tabular_data = tf.convert_to_tensor(tabular_data, dtype=tf.float32)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, tabular_data))\n",
    "    ds = ds.map(preprocess_test, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# ---------- 7. CREATE TEST DATASET ----------\n",
    "test_ds = create_test_dataset(X_img_test_paths, X_tab_test)\n",
    "\n",
    "# Debug one batch\n",
    "for images, tabular in test_ds.take(1):\n",
    "    print(\"✅ Batch shapes:\")\n",
    "    print(\"Image batch:\", images.shape)        # (batch_size, 224, 224, 1)\n",
    "    print(\"Tabular batch:\", tabular.shape)      # (batch_size, NUM_TABULAR_FEATURES)\n",
    "\n",
    "# ---------- 8. LOAD MODEL ----------\n",
    "model = tf.keras.models.load_model(\"best_model.keras\", compile=False)\n",
    "\n",
    "\n",
    "# Re-map the dataset to feed inputs as [image, tabular] list\n",
    "test_ds_for_pred = test_ds.map(lambda img, tab: (img, tab))\n",
    "\n",
    "# Predict\n",
    "preds = model.predict(test_ds_for_pred, verbose=1).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 10. AGGREGATE ----------\n",
    "submission_df = pd.DataFrame({\n",
    "    \"prediction_id\": prediction_ids,\n",
    "    \"cancer\": preds\n",
    "})\n",
    "submission_df = submission_df.groupby(\"prediction_id\", as_index=False).mean()\n",
    "\n",
    "# ---------- 11. SAVE ----------\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv is saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
